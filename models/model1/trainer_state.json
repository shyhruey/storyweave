{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.1548586914440573,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001548586914440573,
      "grad_norm": 2.2712748050689697,
      "learning_rate": 1e-05,
      "loss": 4.0199,
      "step": 1
    },
    {
      "epoch": 0.003097173828881146,
      "grad_norm": 2.573192596435547,
      "learning_rate": 2e-05,
      "loss": 4.0827,
      "step": 2
    },
    {
      "epoch": 0.004645760743321719,
      "grad_norm": 2.7346770763397217,
      "learning_rate": 3e-05,
      "loss": 4.1904,
      "step": 3
    },
    {
      "epoch": 0.006194347657762292,
      "grad_norm": 2.356823444366455,
      "learning_rate": 4e-05,
      "loss": 4.0404,
      "step": 4
    },
    {
      "epoch": 0.007742934572202865,
      "grad_norm": NaN,
      "learning_rate": 4e-05,
      "loss": 3.9132,
      "step": 5
    },
    {
      "epoch": 0.009291521486643438,
      "grad_norm": 2.918201208114624,
      "learning_rate": 5e-05,
      "loss": 3.9236,
      "step": 6
    },
    {
      "epoch": 0.01084010840108401,
      "grad_norm": 2.337618589401245,
      "learning_rate": 6e-05,
      "loss": 3.8556,
      "step": 7
    },
    {
      "epoch": 0.012388695315524584,
      "grad_norm": 2.8909881114959717,
      "learning_rate": 7.000000000000001e-05,
      "loss": 4.1116,
      "step": 8
    },
    {
      "epoch": 0.013937282229965157,
      "grad_norm": 2.5625064373016357,
      "learning_rate": 8e-05,
      "loss": 4.2251,
      "step": 9
    },
    {
      "epoch": 0.01548586914440573,
      "grad_norm": 2.098289966583252,
      "learning_rate": 8.999999999999999e-05,
      "loss": 3.8476,
      "step": 10
    },
    {
      "epoch": 0.017034456058846303,
      "grad_norm": 2.7923128604888916,
      "learning_rate": 0.0001,
      "loss": 4.0938,
      "step": 11
    },
    {
      "epoch": 0.018583042973286876,
      "grad_norm": 2.397801637649536,
      "learning_rate": 0.00011,
      "loss": 4.3081,
      "step": 12
    },
    {
      "epoch": 0.02013162988772745,
      "grad_norm": 2.6080541610717773,
      "learning_rate": 0.00012,
      "loss": 4.0591,
      "step": 13
    },
    {
      "epoch": 0.02168021680216802,
      "grad_norm": 2.4284000396728516,
      "learning_rate": 0.00013000000000000002,
      "loss": 4.0834,
      "step": 14
    },
    {
      "epoch": 0.023228803716608595,
      "grad_norm": 2.761564254760742,
      "learning_rate": 0.00014000000000000001,
      "loss": 4.0154,
      "step": 15
    },
    {
      "epoch": 0.024777390631049168,
      "grad_norm": 2.9129602909088135,
      "learning_rate": 0.00015,
      "loss": 3.9822,
      "step": 16
    },
    {
      "epoch": 0.02632597754548974,
      "grad_norm": 2.801936149597168,
      "learning_rate": 0.00016,
      "loss": 3.8158,
      "step": 17
    },
    {
      "epoch": 0.027874564459930314,
      "grad_norm": 2.938594102859497,
      "learning_rate": 0.00017,
      "loss": 4.0289,
      "step": 18
    },
    {
      "epoch": 0.029423151374370887,
      "grad_norm": 2.6094791889190674,
      "learning_rate": 0.00017999999999999998,
      "loss": 3.786,
      "step": 19
    },
    {
      "epoch": 0.03097173828881146,
      "grad_norm": 2.694938898086548,
      "learning_rate": 0.00019,
      "loss": 3.7397,
      "step": 20
    },
    {
      "epoch": 0.032520325203252036,
      "grad_norm": 3.116546154022217,
      "learning_rate": 0.0002,
      "loss": 3.9404,
      "step": 21
    },
    {
      "epoch": 0.034068912117692605,
      "grad_norm": 2.8673465251922607,
      "learning_rate": 0.00021,
      "loss": 3.9696,
      "step": 22
    },
    {
      "epoch": 0.03561749903213318,
      "grad_norm": 3.7901647090911865,
      "learning_rate": 0.00022,
      "loss": 4.0231,
      "step": 23
    },
    {
      "epoch": 0.03716608594657375,
      "grad_norm": 3.4411325454711914,
      "learning_rate": 0.00023,
      "loss": 4.0635,
      "step": 24
    },
    {
      "epoch": 0.03871467286101433,
      "grad_norm": 2.3530962467193604,
      "learning_rate": 0.00024,
      "loss": 3.9131,
      "step": 25
    },
    {
      "epoch": 0.0402632597754549,
      "grad_norm": 3.4845354557037354,
      "learning_rate": 0.00025,
      "loss": 3.8993,
      "step": 26
    },
    {
      "epoch": 0.041811846689895474,
      "grad_norm": 3.6328985691070557,
      "learning_rate": 0.00026000000000000003,
      "loss": 3.5628,
      "step": 27
    },
    {
      "epoch": 0.04336043360433604,
      "grad_norm": 2.7756729125976562,
      "learning_rate": 0.00027,
      "loss": 3.8166,
      "step": 28
    },
    {
      "epoch": 0.04490902051877662,
      "grad_norm": 3.022740602493286,
      "learning_rate": 0.00028000000000000003,
      "loss": 3.8088,
      "step": 29
    },
    {
      "epoch": 0.04645760743321719,
      "grad_norm": 2.80830979347229,
      "learning_rate": 0.00029,
      "loss": 3.586,
      "step": 30
    },
    {
      "epoch": 0.048006194347657766,
      "grad_norm": 2.594611167907715,
      "learning_rate": 0.0003,
      "loss": 3.6714,
      "step": 31
    },
    {
      "epoch": 0.049554781262098335,
      "grad_norm": 2.795097827911377,
      "learning_rate": 0.00031,
      "loss": 3.8592,
      "step": 32
    },
    {
      "epoch": 0.05110336817653891,
      "grad_norm": 2.7106523513793945,
      "learning_rate": 0.00032,
      "loss": 3.786,
      "step": 33
    },
    {
      "epoch": 0.05265195509097948,
      "grad_norm": 3.0977776050567627,
      "learning_rate": 0.00033,
      "loss": 3.8801,
      "step": 34
    },
    {
      "epoch": 0.05420054200542006,
      "grad_norm": 3.549781560897827,
      "learning_rate": 0.00034,
      "loss": 3.6239,
      "step": 35
    },
    {
      "epoch": 0.05574912891986063,
      "grad_norm": 2.650275468826294,
      "learning_rate": 0.00035,
      "loss": 3.6683,
      "step": 36
    },
    {
      "epoch": 0.057297715834301204,
      "grad_norm": 2.5121774673461914,
      "learning_rate": 0.00035999999999999997,
      "loss": 3.9499,
      "step": 37
    },
    {
      "epoch": 0.05884630274874177,
      "grad_norm": 2.9481201171875,
      "learning_rate": 0.00037,
      "loss": 3.6094,
      "step": 38
    },
    {
      "epoch": 0.06039488966318235,
      "grad_norm": 2.643691301345825,
      "learning_rate": 0.00038,
      "loss": 3.5716,
      "step": 39
    },
    {
      "epoch": 0.06194347657762292,
      "grad_norm": 2.669092893600464,
      "learning_rate": 0.00039000000000000005,
      "loss": 3.6067,
      "step": 40
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 2.7721831798553467,
      "learning_rate": 0.0004,
      "loss": 3.6154,
      "step": 41
    },
    {
      "epoch": 0.06504065040650407,
      "grad_norm": 2.5574541091918945,
      "learning_rate": 0.00041,
      "loss": 3.6612,
      "step": 42
    },
    {
      "epoch": 0.06658923732094464,
      "grad_norm": 2.261216878890991,
      "learning_rate": 0.00042,
      "loss": 3.5125,
      "step": 43
    },
    {
      "epoch": 0.06813782423538521,
      "grad_norm": 2.6085104942321777,
      "learning_rate": 0.00043,
      "loss": 3.5014,
      "step": 44
    },
    {
      "epoch": 0.06968641114982578,
      "grad_norm": 3.4369399547576904,
      "learning_rate": 0.00044,
      "loss": 3.4709,
      "step": 45
    },
    {
      "epoch": 0.07123499806426636,
      "grad_norm": 2.6460793018341064,
      "learning_rate": 0.00045000000000000004,
      "loss": 3.669,
      "step": 46
    },
    {
      "epoch": 0.07278358497870693,
      "grad_norm": 2.778123378753662,
      "learning_rate": 0.00046,
      "loss": 3.4263,
      "step": 47
    },
    {
      "epoch": 0.0743321718931475,
      "grad_norm": 2.346176862716675,
      "learning_rate": 0.00047,
      "loss": 3.3673,
      "step": 48
    },
    {
      "epoch": 0.07588075880758807,
      "grad_norm": 2.386833906173706,
      "learning_rate": 0.00048,
      "loss": 3.5486,
      "step": 49
    },
    {
      "epoch": 0.07742934572202866,
      "grad_norm": 2.5875205993652344,
      "learning_rate": 0.00049,
      "loss": 3.5247,
      "step": 50
    },
    {
      "epoch": 0.07897793263646923,
      "grad_norm": 3.01904034614563,
      "learning_rate": 0.0005,
      "loss": 3.4052,
      "step": 51
    },
    {
      "epoch": 0.0805265195509098,
      "grad_norm": 2.9114370346069336,
      "learning_rate": 0.00051,
      "loss": 3.3793,
      "step": 52
    },
    {
      "epoch": 0.08207510646535036,
      "grad_norm": 2.581815719604492,
      "learning_rate": 0.0005200000000000001,
      "loss": 3.3879,
      "step": 53
    },
    {
      "epoch": 0.08362369337979095,
      "grad_norm": 2.5782411098480225,
      "learning_rate": 0.0005300000000000001,
      "loss": 3.4306,
      "step": 54
    },
    {
      "epoch": 0.08517228029423152,
      "grad_norm": 3.1053714752197266,
      "learning_rate": 0.00054,
      "loss": 3.2264,
      "step": 55
    },
    {
      "epoch": 0.08672086720867209,
      "grad_norm": 2.6063153743743896,
      "learning_rate": 0.00055,
      "loss": 3.2941,
      "step": 56
    },
    {
      "epoch": 0.08826945412311266,
      "grad_norm": 2.864686965942383,
      "learning_rate": 0.0005600000000000001,
      "loss": 3.3459,
      "step": 57
    },
    {
      "epoch": 0.08981804103755324,
      "grad_norm": 2.746039628982544,
      "learning_rate": 0.00057,
      "loss": 3.3531,
      "step": 58
    },
    {
      "epoch": 0.09136662795199381,
      "grad_norm": 3.096391439437866,
      "learning_rate": 0.00058,
      "loss": 3.5045,
      "step": 59
    },
    {
      "epoch": 0.09291521486643438,
      "grad_norm": 2.966925859451294,
      "learning_rate": 0.00059,
      "loss": 3.1852,
      "step": 60
    },
    {
      "epoch": 0.09446380178087495,
      "grad_norm": 2.8416495323181152,
      "learning_rate": 0.0006,
      "loss": 3.2633,
      "step": 61
    },
    {
      "epoch": 0.09601238869531553,
      "grad_norm": 2.724459409713745,
      "learning_rate": 0.00061,
      "loss": 3.2248,
      "step": 62
    },
    {
      "epoch": 0.0975609756097561,
      "grad_norm": 3.417354106903076,
      "learning_rate": 0.00062,
      "loss": 3.3918,
      "step": 63
    },
    {
      "epoch": 0.09910956252419667,
      "grad_norm": 2.818500280380249,
      "learning_rate": 0.00063,
      "loss": 3.0984,
      "step": 64
    },
    {
      "epoch": 0.10065814943863724,
      "grad_norm": 3.4029359817504883,
      "learning_rate": 0.00064,
      "loss": 3.1406,
      "step": 65
    },
    {
      "epoch": 0.10220673635307782,
      "grad_norm": 2.523703098297119,
      "learning_rate": 0.0006500000000000001,
      "loss": 3.2779,
      "step": 66
    },
    {
      "epoch": 0.10375532326751839,
      "grad_norm": 2.7163376808166504,
      "learning_rate": 0.00066,
      "loss": 3.2283,
      "step": 67
    },
    {
      "epoch": 0.10530391018195896,
      "grad_norm": 3.3474528789520264,
      "learning_rate": 0.00067,
      "loss": 3.2846,
      "step": 68
    },
    {
      "epoch": 0.10685249709639953,
      "grad_norm": 2.723794460296631,
      "learning_rate": 0.00068,
      "loss": 3.4612,
      "step": 69
    },
    {
      "epoch": 0.10840108401084012,
      "grad_norm": 2.5084362030029297,
      "learning_rate": 0.00069,
      "loss": 3.2681,
      "step": 70
    },
    {
      "epoch": 0.10994967092528068,
      "grad_norm": 2.5534729957580566,
      "learning_rate": 0.0007,
      "loss": 3.22,
      "step": 71
    },
    {
      "epoch": 0.11149825783972125,
      "grad_norm": 2.616774082183838,
      "learning_rate": 0.00071,
      "loss": 3.1306,
      "step": 72
    },
    {
      "epoch": 0.11304684475416182,
      "grad_norm": 3.4579598903656006,
      "learning_rate": 0.0007199999999999999,
      "loss": 3.2349,
      "step": 73
    },
    {
      "epoch": 0.11459543166860241,
      "grad_norm": 2.5795395374298096,
      "learning_rate": 0.00073,
      "loss": 3.1572,
      "step": 74
    },
    {
      "epoch": 0.11614401858304298,
      "grad_norm": 2.3649160861968994,
      "learning_rate": 0.00074,
      "loss": 3.1036,
      "step": 75
    },
    {
      "epoch": 0.11769260549748355,
      "grad_norm": 2.4818832874298096,
      "learning_rate": 0.00075,
      "loss": 3.2273,
      "step": 76
    },
    {
      "epoch": 0.11924119241192412,
      "grad_norm": 2.6315505504608154,
      "learning_rate": 0.00076,
      "loss": 3.2383,
      "step": 77
    },
    {
      "epoch": 0.1207897793263647,
      "grad_norm": 2.254838228225708,
      "learning_rate": 0.0007700000000000001,
      "loss": 3.0448,
      "step": 78
    },
    {
      "epoch": 0.12233836624080527,
      "grad_norm": 2.275200605392456,
      "learning_rate": 0.0007800000000000001,
      "loss": 2.9952,
      "step": 79
    },
    {
      "epoch": 0.12388695315524584,
      "grad_norm": 3.271662473678589,
      "learning_rate": 0.00079,
      "loss": 3.3412,
      "step": 80
    },
    {
      "epoch": 0.1254355400696864,
      "grad_norm": 3.111178159713745,
      "learning_rate": 0.0008,
      "loss": 3.2386,
      "step": 81
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 2.466949939727783,
      "learning_rate": 0.0008100000000000001,
      "loss": 3.304,
      "step": 82
    },
    {
      "epoch": 0.12853271389856755,
      "grad_norm": 2.510289192199707,
      "learning_rate": 0.00082,
      "loss": 3.328,
      "step": 83
    },
    {
      "epoch": 0.13008130081300814,
      "grad_norm": 2.753089666366577,
      "learning_rate": 0.00083,
      "loss": 3.2428,
      "step": 84
    },
    {
      "epoch": 0.1316298877274487,
      "grad_norm": 2.8540310859680176,
      "learning_rate": 0.00084,
      "loss": 3.2647,
      "step": 85
    },
    {
      "epoch": 0.13317847464188928,
      "grad_norm": 2.6644091606140137,
      "learning_rate": 0.00085,
      "loss": 2.9958,
      "step": 86
    },
    {
      "epoch": 0.13472706155632985,
      "grad_norm": 2.8562097549438477,
      "learning_rate": 0.00086,
      "loss": 3.4921,
      "step": 87
    },
    {
      "epoch": 0.13627564847077042,
      "grad_norm": 2.2044458389282227,
      "learning_rate": 0.00087,
      "loss": 3.0465,
      "step": 88
    },
    {
      "epoch": 0.137824235385211,
      "grad_norm": 2.147697925567627,
      "learning_rate": 0.00088,
      "loss": 3.127,
      "step": 89
    },
    {
      "epoch": 0.13937282229965156,
      "grad_norm": 2.645735025405884,
      "learning_rate": 0.0008900000000000001,
      "loss": 3.2065,
      "step": 90
    },
    {
      "epoch": 0.14092140921409213,
      "grad_norm": 2.6330406665802,
      "learning_rate": 0.0009000000000000001,
      "loss": 3.1485,
      "step": 91
    },
    {
      "epoch": 0.14246999612853273,
      "grad_norm": 2.6693103313446045,
      "learning_rate": 0.00091,
      "loss": 3.2877,
      "step": 92
    },
    {
      "epoch": 0.1440185830429733,
      "grad_norm": 2.898367166519165,
      "learning_rate": 0.00092,
      "loss": 3.1899,
      "step": 93
    },
    {
      "epoch": 0.14556716995741387,
      "grad_norm": 2.9135947227478027,
      "learning_rate": 0.00093,
      "loss": 3.2352,
      "step": 94
    },
    {
      "epoch": 0.14711575687185444,
      "grad_norm": 2.4965765476226807,
      "learning_rate": 0.00094,
      "loss": 3.1234,
      "step": 95
    },
    {
      "epoch": 0.148664343786295,
      "grad_norm": 2.3542473316192627,
      "learning_rate": 0.00095,
      "loss": 3.2288,
      "step": 96
    },
    {
      "epoch": 0.15021293070073558,
      "grad_norm": 2.455073118209839,
      "learning_rate": 0.00096,
      "loss": 3.2611,
      "step": 97
    },
    {
      "epoch": 0.15176151761517614,
      "grad_norm": 2.230440378189087,
      "learning_rate": 0.0009699999999999999,
      "loss": 3.1995,
      "step": 98
    },
    {
      "epoch": 0.15331010452961671,
      "grad_norm": 2.3054039478302,
      "learning_rate": 0.00098,
      "loss": 3.3749,
      "step": 99
    },
    {
      "epoch": 0.1548586914440573,
      "grad_norm": 2.492161750793457,
      "learning_rate": 0.00099,
      "loss": 3.143,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 613464883200000.0,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
