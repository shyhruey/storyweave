{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.01986294567484358,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00019862945674843578,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 4.1118,
      "step": 1
    },
    {
      "epoch": 0.00039725891349687157,
      "grad_norm": 4.0052995681762695,
      "learning_rate": 1e-05,
      "loss": 3.9065,
      "step": 2
    },
    {
      "epoch": 0.0005958883702453074,
      "grad_norm": NaN,
      "learning_rate": 1e-05,
      "loss": 3.9037,
      "step": 3
    },
    {
      "epoch": 0.0007945178269937431,
      "grad_norm": 4.5696120262146,
      "learning_rate": 2e-05,
      "loss": 4.2685,
      "step": 4
    },
    {
      "epoch": 0.000993147283742179,
      "grad_norm": 5.146114826202393,
      "learning_rate": 3e-05,
      "loss": 4.1564,
      "step": 5
    },
    {
      "epoch": 0.0011917767404906149,
      "grad_norm": 4.860050201416016,
      "learning_rate": 4e-05,
      "loss": 4.2269,
      "step": 6
    },
    {
      "epoch": 0.0013904061972390505,
      "grad_norm": 4.1809892654418945,
      "learning_rate": 5e-05,
      "loss": 3.9499,
      "step": 7
    },
    {
      "epoch": 0.0015890356539874863,
      "grad_norm": 5.408351421356201,
      "learning_rate": 6e-05,
      "loss": 3.9699,
      "step": 8
    },
    {
      "epoch": 0.001787665110735922,
      "grad_norm": 5.188339710235596,
      "learning_rate": 7.000000000000001e-05,
      "loss": 3.946,
      "step": 9
    },
    {
      "epoch": 0.001986294567484358,
      "grad_norm": 4.5393829345703125,
      "learning_rate": 8e-05,
      "loss": 3.9793,
      "step": 10
    },
    {
      "epoch": 0.0021849240242327937,
      "grad_norm": 6.442137718200684,
      "learning_rate": 8.999999999999999e-05,
      "loss": 4.0703,
      "step": 11
    },
    {
      "epoch": 0.0023835534809812297,
      "grad_norm": 6.699929237365723,
      "learning_rate": 0.0001,
      "loss": 3.9745,
      "step": 12
    },
    {
      "epoch": 0.0025821829377296653,
      "grad_norm": 5.871596336364746,
      "learning_rate": 0.00011,
      "loss": 3.9957,
      "step": 13
    },
    {
      "epoch": 0.002780812394478101,
      "grad_norm": 7.142533302307129,
      "learning_rate": 0.00012,
      "loss": 3.9332,
      "step": 14
    },
    {
      "epoch": 0.002979441851226537,
      "grad_norm": 5.1679816246032715,
      "learning_rate": 0.00013000000000000002,
      "loss": 3.9941,
      "step": 15
    },
    {
      "epoch": 0.0031780713079749725,
      "grad_norm": 6.040624141693115,
      "learning_rate": 0.00014000000000000001,
      "loss": 3.8982,
      "step": 16
    },
    {
      "epoch": 0.0033767007647234086,
      "grad_norm": 4.0360941886901855,
      "learning_rate": 0.00015,
      "loss": 3.9482,
      "step": 17
    },
    {
      "epoch": 0.003575330221471844,
      "grad_norm": 7.26976203918457,
      "learning_rate": 0.00016,
      "loss": 3.9792,
      "step": 18
    },
    {
      "epoch": 0.00377395967822028,
      "grad_norm": 6.180200576782227,
      "learning_rate": 0.00017,
      "loss": 3.8698,
      "step": 19
    },
    {
      "epoch": 0.003972589134968716,
      "grad_norm": 5.751336574554443,
      "learning_rate": 0.00017999999999999998,
      "loss": 3.8694,
      "step": 20
    },
    {
      "epoch": 0.004171218591717152,
      "grad_norm": 8.239203453063965,
      "learning_rate": 0.00019,
      "loss": 3.7333,
      "step": 21
    },
    {
      "epoch": 0.004369848048465587,
      "grad_norm": 6.020153999328613,
      "learning_rate": 0.0002,
      "loss": 3.8774,
      "step": 22
    },
    {
      "epoch": 0.004568477505214023,
      "grad_norm": 7.800376892089844,
      "learning_rate": 0.00021,
      "loss": 3.7539,
      "step": 23
    },
    {
      "epoch": 0.0047671069619624595,
      "grad_norm": 6.80797815322876,
      "learning_rate": 0.00022,
      "loss": 3.606,
      "step": 24
    },
    {
      "epoch": 0.004965736418710895,
      "grad_norm": 5.1517486572265625,
      "learning_rate": 0.00023,
      "loss": 3.6774,
      "step": 25
    },
    {
      "epoch": 0.005164365875459331,
      "grad_norm": 5.963303565979004,
      "learning_rate": 0.00024,
      "loss": 3.6321,
      "step": 26
    },
    {
      "epoch": 0.005362995332207766,
      "grad_norm": 5.40075159072876,
      "learning_rate": 0.00025,
      "loss": 3.5276,
      "step": 27
    },
    {
      "epoch": 0.005561624788956202,
      "grad_norm": 6.598555088043213,
      "learning_rate": 0.00026000000000000003,
      "loss": 3.6813,
      "step": 28
    },
    {
      "epoch": 0.005760254245704638,
      "grad_norm": 5.5816192626953125,
      "learning_rate": 0.00027,
      "loss": 3.6203,
      "step": 29
    },
    {
      "epoch": 0.005958883702453074,
      "grad_norm": 6.39817476272583,
      "learning_rate": 0.00028000000000000003,
      "loss": 3.5997,
      "step": 30
    },
    {
      "epoch": 0.0061575131592015095,
      "grad_norm": 5.320666790008545,
      "learning_rate": 0.00029,
      "loss": 3.4731,
      "step": 31
    },
    {
      "epoch": 0.006356142615949945,
      "grad_norm": 5.438331127166748,
      "learning_rate": 0.0003,
      "loss": 3.5913,
      "step": 32
    },
    {
      "epoch": 0.0065547720726983815,
      "grad_norm": 4.682440280914307,
      "learning_rate": 0.00031,
      "loss": 3.4809,
      "step": 33
    },
    {
      "epoch": 0.006753401529446817,
      "grad_norm": 5.537029266357422,
      "learning_rate": 0.00032,
      "loss": 3.2217,
      "step": 34
    },
    {
      "epoch": 0.006952030986195253,
      "grad_norm": 5.929473876953125,
      "learning_rate": 0.00033,
      "loss": 3.4943,
      "step": 35
    },
    {
      "epoch": 0.007150660442943688,
      "grad_norm": 4.848569393157959,
      "learning_rate": 0.00034,
      "loss": 3.3785,
      "step": 36
    },
    {
      "epoch": 0.007349289899692124,
      "grad_norm": 3.752530574798584,
      "learning_rate": 0.00035,
      "loss": 3.5869,
      "step": 37
    },
    {
      "epoch": 0.00754791935644056,
      "grad_norm": 6.272673606872559,
      "learning_rate": 0.00035999999999999997,
      "loss": 3.1837,
      "step": 38
    },
    {
      "epoch": 0.007746548813188996,
      "grad_norm": 5.161250114440918,
      "learning_rate": 0.00037,
      "loss": 3.2122,
      "step": 39
    },
    {
      "epoch": 0.007945178269937432,
      "grad_norm": 5.668829441070557,
      "learning_rate": 0.00038,
      "loss": 3.007,
      "step": 40
    },
    {
      "epoch": 0.008143807726685867,
      "grad_norm": 5.853567600250244,
      "learning_rate": 0.00039000000000000005,
      "loss": 3.0864,
      "step": 41
    },
    {
      "epoch": 0.008342437183434304,
      "grad_norm": 6.422684192657471,
      "learning_rate": 0.0004,
      "loss": 3.0885,
      "step": 42
    },
    {
      "epoch": 0.008541066640182738,
      "grad_norm": 5.372076034545898,
      "learning_rate": 0.00041,
      "loss": 2.9451,
      "step": 43
    },
    {
      "epoch": 0.008739696096931175,
      "grad_norm": 4.637569427490234,
      "learning_rate": 0.00042,
      "loss": 3.0264,
      "step": 44
    },
    {
      "epoch": 0.008938325553679611,
      "grad_norm": 5.921586036682129,
      "learning_rate": 0.00043,
      "loss": 2.7504,
      "step": 45
    },
    {
      "epoch": 0.009136955010428046,
      "grad_norm": 5.916018486022949,
      "learning_rate": 0.00044,
      "loss": 2.859,
      "step": 46
    },
    {
      "epoch": 0.009335584467176482,
      "grad_norm": 5.842635154724121,
      "learning_rate": 0.00045000000000000004,
      "loss": 2.9124,
      "step": 47
    },
    {
      "epoch": 0.009534213923924919,
      "grad_norm": 6.5540313720703125,
      "learning_rate": 0.00046,
      "loss": 2.8931,
      "step": 48
    },
    {
      "epoch": 0.009732843380673354,
      "grad_norm": 7.5341715812683105,
      "learning_rate": 0.00047,
      "loss": 2.7592,
      "step": 49
    },
    {
      "epoch": 0.00993147283742179,
      "grad_norm": 6.392964839935303,
      "learning_rate": 0.00048,
      "loss": 2.6779,
      "step": 50
    },
    {
      "epoch": 0.010130102294170225,
      "grad_norm": 6.636052131652832,
      "learning_rate": 0.00049,
      "loss": 3.014,
      "step": 51
    },
    {
      "epoch": 0.010328731750918661,
      "grad_norm": 5.025267601013184,
      "learning_rate": 0.0005,
      "loss": 2.9201,
      "step": 52
    },
    {
      "epoch": 0.010527361207667098,
      "grad_norm": 5.633594512939453,
      "learning_rate": 0.00051,
      "loss": 2.7388,
      "step": 53
    },
    {
      "epoch": 0.010725990664415532,
      "grad_norm": 6.656038284301758,
      "learning_rate": 0.0005200000000000001,
      "loss": 2.731,
      "step": 54
    },
    {
      "epoch": 0.010924620121163969,
      "grad_norm": 5.178544044494629,
      "learning_rate": 0.0005300000000000001,
      "loss": 2.5724,
      "step": 55
    },
    {
      "epoch": 0.011123249577912404,
      "grad_norm": 5.237236499786377,
      "learning_rate": 0.00054,
      "loss": 2.6854,
      "step": 56
    },
    {
      "epoch": 0.01132187903466084,
      "grad_norm": 4.047842502593994,
      "learning_rate": 0.00055,
      "loss": 2.8252,
      "step": 57
    },
    {
      "epoch": 0.011520508491409277,
      "grad_norm": 7.273922443389893,
      "learning_rate": 0.0005600000000000001,
      "loss": 2.5535,
      "step": 58
    },
    {
      "epoch": 0.011719137948157711,
      "grad_norm": 6.6852850914001465,
      "learning_rate": 0.00057,
      "loss": 2.7373,
      "step": 59
    },
    {
      "epoch": 0.011917767404906148,
      "grad_norm": 7.380562782287598,
      "learning_rate": 0.00058,
      "loss": 2.5624,
      "step": 60
    },
    {
      "epoch": 0.012116396861654583,
      "grad_norm": 5.072927474975586,
      "learning_rate": 0.00059,
      "loss": 3.0049,
      "step": 61
    },
    {
      "epoch": 0.012315026318403019,
      "grad_norm": 3.9453911781311035,
      "learning_rate": 0.0006,
      "loss": 2.8836,
      "step": 62
    },
    {
      "epoch": 0.012513655775151455,
      "grad_norm": 4.443178653717041,
      "learning_rate": 0.00061,
      "loss": 2.7461,
      "step": 63
    },
    {
      "epoch": 0.01271228523189989,
      "grad_norm": 4.092959880828857,
      "learning_rate": 0.00062,
      "loss": 2.4895,
      "step": 64
    },
    {
      "epoch": 0.012910914688648327,
      "grad_norm": 3.567047357559204,
      "learning_rate": 0.00063,
      "loss": 2.5871,
      "step": 65
    },
    {
      "epoch": 0.013109544145396763,
      "grad_norm": 3.726463556289673,
      "learning_rate": 0.00064,
      "loss": 2.5998,
      "step": 66
    },
    {
      "epoch": 0.013308173602145198,
      "grad_norm": 3.771263360977173,
      "learning_rate": 0.0006500000000000001,
      "loss": 2.5817,
      "step": 67
    },
    {
      "epoch": 0.013506803058893634,
      "grad_norm": 4.198367118835449,
      "learning_rate": 0.00066,
      "loss": 2.2609,
      "step": 68
    },
    {
      "epoch": 0.013705432515642069,
      "grad_norm": 4.1087141036987305,
      "learning_rate": 0.00067,
      "loss": 2.5228,
      "step": 69
    },
    {
      "epoch": 0.013904061972390505,
      "grad_norm": 3.657780170440674,
      "learning_rate": 0.00068,
      "loss": 2.4795,
      "step": 70
    },
    {
      "epoch": 0.014102691429138942,
      "grad_norm": 3.0624871253967285,
      "learning_rate": 0.00069,
      "loss": 2.3527,
      "step": 71
    },
    {
      "epoch": 0.014301320885887377,
      "grad_norm": 4.306761264801025,
      "learning_rate": 0.0007,
      "loss": 2.5351,
      "step": 72
    },
    {
      "epoch": 0.014499950342635813,
      "grad_norm": 3.967081308364868,
      "learning_rate": 0.00071,
      "loss": 2.4232,
      "step": 73
    },
    {
      "epoch": 0.014698579799384248,
      "grad_norm": 3.942368507385254,
      "learning_rate": 0.0007199999999999999,
      "loss": 2.8521,
      "step": 74
    },
    {
      "epoch": 0.014897209256132684,
      "grad_norm": 3.703447103500366,
      "learning_rate": 0.00073,
      "loss": 2.6418,
      "step": 75
    },
    {
      "epoch": 0.01509583871288112,
      "grad_norm": 3.3349874019622803,
      "learning_rate": 0.00074,
      "loss": 2.4257,
      "step": 76
    },
    {
      "epoch": 0.015294468169629555,
      "grad_norm": 3.614016532897949,
      "learning_rate": 0.00075,
      "loss": 2.9145,
      "step": 77
    },
    {
      "epoch": 0.015493097626377992,
      "grad_norm": 3.4741742610931396,
      "learning_rate": 0.00076,
      "loss": 2.3907,
      "step": 78
    },
    {
      "epoch": 0.01569172708312643,
      "grad_norm": 3.868960380554199,
      "learning_rate": 0.0007700000000000001,
      "loss": 2.9029,
      "step": 79
    },
    {
      "epoch": 0.015890356539874865,
      "grad_norm": 3.558326482772827,
      "learning_rate": 0.0007800000000000001,
      "loss": 2.933,
      "step": 80
    },
    {
      "epoch": 0.016088985996623298,
      "grad_norm": 3.51611590385437,
      "learning_rate": 0.00079,
      "loss": 2.7708,
      "step": 81
    },
    {
      "epoch": 0.016287615453371734,
      "grad_norm": 3.7186408042907715,
      "learning_rate": 0.0008,
      "loss": 2.3372,
      "step": 82
    },
    {
      "epoch": 0.01648624491012017,
      "grad_norm": 5.15639591217041,
      "learning_rate": 0.0008100000000000001,
      "loss": 2.4144,
      "step": 83
    },
    {
      "epoch": 0.016684874366868607,
      "grad_norm": 3.1665725708007812,
      "learning_rate": 0.00082,
      "loss": 2.5356,
      "step": 84
    },
    {
      "epoch": 0.016883503823617044,
      "grad_norm": 3.2115206718444824,
      "learning_rate": 0.00083,
      "loss": 2.5405,
      "step": 85
    },
    {
      "epoch": 0.017082133280365477,
      "grad_norm": 3.3262083530426025,
      "learning_rate": 0.00084,
      "loss": 2.7389,
      "step": 86
    },
    {
      "epoch": 0.017280762737113913,
      "grad_norm": 3.719749689102173,
      "learning_rate": 0.00085,
      "loss": 2.2177,
      "step": 87
    },
    {
      "epoch": 0.01747939219386235,
      "grad_norm": 2.8791019916534424,
      "learning_rate": 0.00086,
      "loss": 2.7683,
      "step": 88
    },
    {
      "epoch": 0.017678021650610786,
      "grad_norm": 3.8822591304779053,
      "learning_rate": 0.00087,
      "loss": 2.7311,
      "step": 89
    },
    {
      "epoch": 0.017876651107359223,
      "grad_norm": 4.497526168823242,
      "learning_rate": 0.00088,
      "loss": 2.7138,
      "step": 90
    },
    {
      "epoch": 0.018075280564107656,
      "grad_norm": 3.9752707481384277,
      "learning_rate": 0.0008900000000000001,
      "loss": 2.545,
      "step": 91
    },
    {
      "epoch": 0.018273910020856092,
      "grad_norm": 3.8566653728485107,
      "learning_rate": 0.0009000000000000001,
      "loss": 2.3074,
      "step": 92
    },
    {
      "epoch": 0.01847253947760453,
      "grad_norm": 3.3883330821990967,
      "learning_rate": 0.00091,
      "loss": 2.8857,
      "step": 93
    },
    {
      "epoch": 0.018671168934352965,
      "grad_norm": 4.35073709487915,
      "learning_rate": 0.00092,
      "loss": 2.1715,
      "step": 94
    },
    {
      "epoch": 0.0188697983911014,
      "grad_norm": 4.19511079788208,
      "learning_rate": 0.00093,
      "loss": 2.5535,
      "step": 95
    },
    {
      "epoch": 0.019068427847849838,
      "grad_norm": 3.567692518234253,
      "learning_rate": 0.00094,
      "loss": 2.6458,
      "step": 96
    },
    {
      "epoch": 0.01926705730459827,
      "grad_norm": 3.3218984603881836,
      "learning_rate": 0.00095,
      "loss": 2.8538,
      "step": 97
    },
    {
      "epoch": 0.019465686761346707,
      "grad_norm": 4.022864818572998,
      "learning_rate": 0.00096,
      "loss": 2.4096,
      "step": 98
    },
    {
      "epoch": 0.019664316218095144,
      "grad_norm": 3.6935207843780518,
      "learning_rate": 0.0009699999999999999,
      "loss": 2.4734,
      "step": 99
    },
    {
      "epoch": 0.01986294567484358,
      "grad_norm": 3.369804620742798,
      "learning_rate": 0.00098,
      "loss": 2.5382,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 613464883200000.0,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
